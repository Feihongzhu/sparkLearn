# Iceberg å¿«é€Ÿå…¥é—¨ - 10 åˆ†é’Ÿä¸Šæ‰‹

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨ 10 åˆ†é’Ÿå†…å¿«é€Ÿä¸Šæ‰‹ Apache Icebergã€‚

## ğŸš€ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# ä¸‹è½½ Iceberg JAR
make iceberg-setup
```

### 2. éªŒè¯ç¯å¢ƒ

```bash
# æ£€æŸ¥ Python å’Œ Spark ç‰ˆæœ¬
python3 --version  # éœ€è¦ 3.8+
python3 -c "import pyspark; print(pyspark.__version__)"  # éœ€è¦ 3.5.0
```

## ğŸ“ ç¬¬äºŒæ­¥ï¼šåˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ª Iceberg è¡¨

### æ–¹å¼ä¸€ï¼šä½¿ç”¨æä¾›çš„ç¤ºä¾‹ï¼ˆæœ€å¿«ï¼‰

```bash
# ç”Ÿæˆæµ‹è¯•æ•°æ®
make data

# è¿è¡Œ Iceberg æ‘„å…¥ä»»åŠ¡ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰
make iceberg-ingest-local
```

### æ–¹å¼äºŒï¼šæ‰‹åŠ¨ç¼–å†™ä»£ç 

åˆ›å»ºæ–‡ä»¶ `my_first_iceberg.py`ï¼š

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# åˆ›å»ºå¸¦ Iceberg æ”¯æŒçš„ Spark Session
spark = (
    SparkSession.builder
    .appName("MyFirstIceberg")
    .master("local[*]")
    .config("spark.sql.extensions", 
            "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.local.type", "hadoop")
    .config("spark.sql.catalog.local.warehouse", "data/iceberg-warehouse")
    .getOrCreate()
)

# åˆ›å»ºç¤ºä¾‹æ•°æ®
data = [
    ("order_001", "user_A", 100.0, "2025-11-01"),
    ("order_002", "user_B", 200.0, "2025-11-02"),
    ("order_003", "user_A", 150.0, "2025-11-03"),
]

df = spark.createDataFrame(data, ["order_id", "user_id", "amount", "order_date"])
df = df.withColumn("order_date", F.to_date("order_date"))

# åˆ›å»º Iceberg è¡¨
spark.sql("CREATE DATABASE IF NOT EXISTS local.my_db")

df.writeTo("local.my_db.orders") \
  .using("iceberg") \
  .create()

print("âœ… Iceberg è¡¨åˆ›å»ºæˆåŠŸï¼")

# è¯»å–æ•°æ®
result = spark.table("local.my_db.orders")
result.show()

spark.stop()
```

è¿è¡Œï¼š

```bash
python3 my_first_iceberg.py
```

## ğŸ” ç¬¬ä¸‰æ­¥ï¼šæ¢ç´¢ Iceberg ç‰¹æ€§

### 1. æŸ¥è¯¢å…ƒæ•°æ®

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("IcebergExplore").getOrCreate()

# æŸ¥çœ‹å¿«ç…§å†å²
spark.sql("SELECT * FROM local.my_db.orders.snapshots").show()

# æŸ¥çœ‹æ•°æ®æ–‡ä»¶
spark.sql("SELECT file_path, record_count FROM local.my_db.orders.files").show()
```

### 2. è¿½åŠ æ•°æ®

```python
# åˆ›å»ºæ–°æ•°æ®
new_data = [
    ("order_004", "user_C", 300.0, "2025-11-04"),
]

new_df = spark.createDataFrame(new_data, ["order_id", "user_id", "amount", "order_date"])
new_df = new_df.withColumn("order_date", F.to_date("order_date"))

# è¿½åŠ åˆ° Iceberg è¡¨
new_df.writeTo("local.my_db.orders").using("iceberg").append()

# éªŒè¯
spark.table("local.my_db.orders").show()
```

### 3. Time Travelï¼ˆæ—¶é—´æ—…è¡Œï¼‰

```python
# è·å–æ‰€æœ‰å¿«ç…§
snapshots = spark.sql("SELECT snapshot_id FROM local.my_db.orders.snapshots")
first_snapshot = snapshots.first()[0]

# æŸ¥è¯¢ç¬¬ä¸€ä¸ªå¿«ç…§çš„æ•°æ®
df_history = spark.read \
    .option("snapshot-id", first_snapshot) \
    .format("iceberg") \
    .load("local.my_db.orders")

print(f"ç¬¬ä¸€ä¸ªå¿«ç…§çš„è®°å½•æ•°: {df_history.count()}")
df_history.show()
```

### 4. Schema Evolutionï¼ˆæ¨¡å¼æ¼”å˜ï¼‰

```python
# æ·»åŠ æ–°åˆ—
spark.sql("ALTER TABLE local.my_db.orders ADD COLUMN status STRING")

# æ›´æ–°æ–°åˆ—
spark.sql("""
    UPDATE local.my_db.orders
    SET status = 'completed'
    WHERE amount > 100
""")

# æŸ¥çœ‹ç»“æœ
spark.sql("SELECT * FROM local.my_db.orders").show()
```

### 5. MERGE INTOï¼ˆåˆå¹¶æ›´æ–°ï¼‰

```python
# åˆ›å»ºæ›´æ–°æ•°æ®
updates = [
    ("order_001", "user_A", 999.0, "2025-11-01", "updated"),
    ("order_999", "user_Z", 500.0, "2025-11-05", "new"),
]

updates_df = spark.createDataFrame(
    updates, 
    ["order_id", "user_id", "amount", "order_date", "status"]
)
updates_df = updates_df.withColumn("order_date", F.to_date("order_date"))

# åˆ›å»ºä¸´æ—¶è§†å›¾
updates_df.createOrReplaceTempView("updates")

# æ‰§è¡Œ MERGE
spark.sql("""
    MERGE INTO local.my_db.orders t
    USING updates u
    ON t.order_id = u.order_id
    WHEN MATCHED THEN
        UPDATE SET 
            t.amount = u.amount,
            t.status = u.status
    WHEN NOT MATCHED THEN
        INSERT *
""")

# æŸ¥çœ‹ç»“æœ
spark.sql("SELECT * FROM local.my_db.orders ORDER BY order_id").show()
```

## ğŸ¯ ç¬¬å››æ­¥ï¼šè¿è¡Œå®Œæ•´æ¼”ç¤º

æˆ‘ä»¬å·²ç»ä¸ºä½ å‡†å¤‡äº†å®Œæ•´çš„ç¤ºä¾‹ä»£ç ï¼š

```bash
# è¿è¡Œå®Œæ•´æ¼”ç¤ºï¼ˆåŒ…å«æ‰€æœ‰ç‰¹æ€§ï¼‰
make iceberg-demo-local

# æˆ–è€…å•ç‹¬è¿è¡Œå„ä¸ªéƒ¨åˆ†
make iceberg-ingest-local       # æ•°æ®æ‘„å…¥
make iceberg-features-local     # é«˜çº§ç‰¹æ€§
```

## ğŸ“Š ç¬¬äº”æ­¥ï¼šæŸ¥çœ‹ç»“æœ

### æŸ¥çœ‹åˆ›å»ºçš„æ–‡ä»¶

```bash
# æŸ¥çœ‹ Iceberg warehouse ç›®å½•ç»“æ„
tree data/iceberg-warehouse/db/orders/

# å…¸å‹çš„ç›®å½•ç»“æ„ï¼š
# data/iceberg-warehouse/db/orders/
# â”œâ”€â”€ metadata/
# â”‚   â”œâ”€â”€ v1.metadata.json      # è¡¨å…ƒæ•°æ®
# â”‚   â”œâ”€â”€ snap-*.avro            # å¿«ç…§æ¸…å•
# â”‚   â””â”€â”€ *.avro                 # manifest æ–‡ä»¶
# â””â”€â”€ data/
#     â””â”€â”€ *.parquet              # å®é™…æ•°æ®æ–‡ä»¶
```

### ä½¿ç”¨ Spark SQL äº¤äº’å¼æŸ¥è¯¢

```bash
# å¯åŠ¨ Spark SQL CLIï¼ˆéœ€è¦å…ˆä¸‹è½½ Iceberg JARï¼‰
spark-sql \
  --jars jars/iceberg-spark-runtime-3.5_2.12-1.4.3.jar \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.local.type=hadoop \
  --conf spark.sql.catalog.local.warehouse=data/iceberg-warehouse

# åœ¨ SQL CLI ä¸­æ‰§è¡ŒæŸ¥è¯¢
spark-sql> USE local.db;
spark-sql> SHOW TABLES;
spark-sql> SELECT * FROM orders LIMIT 10;
spark-sql> SELECT * FROM orders.snapshots;
```

## ğŸ§ª ç¬¬å…­æ­¥ï¼šè¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œ Iceberg å•å…ƒæµ‹è¯•
make iceberg-test

# æˆ–ç›´æ¥ä½¿ç”¨ pytest
pytest tests/test_iceberg.py -v
```

## ğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ 

æ­å–œï¼ä½ å·²ç»å®Œæˆäº† Iceberg å¿«é€Ÿå…¥é—¨ã€‚æ¥ä¸‹æ¥å¯ä»¥ï¼š

1. **æ·±å…¥å­¦ä¹ **ï¼šé˜…è¯» [ICEBERG_GUIDE.md](ICEBERG_GUIDE.md) äº†è§£æ›´å¤šç»†èŠ‚
2. **å®è·µç»ƒä¹ **ï¼š
   - æ„å»ºä¸€ä¸ªå®Œæ•´çš„ ETL ç®¡é“
   - å®ç° CDCï¼ˆChange Data Captureï¼‰åœºæ™¯
   - ä¼˜åŒ–å¤§è§„æ¨¡æ•°æ®è¡¨çš„æ€§èƒ½
3. **æ¢ç´¢é«˜çº§ç‰¹æ€§**ï¼š
   - åˆ†åŒºæ¼”å˜
   - Z-Order æ’åº
   - ä¸ Flink/Trino é›†æˆ

## â“ å¸¸è§é—®é¢˜

### Q1: è¿è¡Œæ—¶æŠ¥é”™ `ClassNotFoundException: org.apache.iceberg...`

**A**: ç¡®ä¿å·²ç»ä¸‹è½½ Iceberg JARï¼š

```bash
make iceberg-setup
```

å¦‚æœä½¿ç”¨è‡ªå·±çš„è„šæœ¬ï¼Œéœ€è¦åœ¨ `spark-submit` æ—¶æŒ‡å®šï¼š

```bash
spark-submit --jars jars/iceberg-spark-runtime-3.5_2.12-1.4.3.jar your_script.py
```

### Q2: å¦‚ä½•åˆ é™¤ Iceberg è¡¨ï¼Ÿ

**A**: ä½¿ç”¨ SQL DROP TABLEï¼š

```python
spark.sql("DROP TABLE local.my_db.orders")
```

### Q3: Iceberg æ•°æ®å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ

**A**: é»˜è®¤å­˜å‚¨åœ¨ `data/iceberg-warehouse/` ç›®å½•ä¸‹ã€‚å¯ä»¥åœ¨é…ç½®ä¸­ä¿®æ”¹ï¼š

```python
.config("spark.sql.catalog.local.warehouse", "ä½ çš„è·¯å¾„")
```

### Q4: å¦‚ä½•æ¸…ç†æµ‹è¯•æ•°æ®ï¼Ÿ

**A**: åˆ é™¤ warehouse ç›®å½•ï¼š

```bash
rm -rf data/iceberg-warehouse
```

### Q5: æ˜¯å¦å¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Ÿ

**A**: å¯ä»¥ï¼Iceberg å·²ç»åœ¨ Netflixã€Appleã€LinkedIn ç­‰å…¬å¸çš„ç”Ÿäº§ç¯å¢ƒä¸­å¹¿æ³›ä½¿ç”¨ã€‚ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼š
- ä½¿ç”¨äº‘ç«¯å¯¹è±¡å­˜å‚¨ï¼ˆS3ã€GCSã€Azure Blobï¼‰
- ä½¿ç”¨ Hive Metastore æˆ– AWS Glue ä½œä¸º catalog
- å®šæœŸè¿è¡Œè¡¨ç»´æŠ¤ä»»åŠ¡ï¼ˆåˆå¹¶å°æ–‡ä»¶ã€æ¸…ç†å¿«ç…§ï¼‰

## ğŸ“ å­¦ä¹ èµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://iceberg.apache.org/
- **æœ¬é¡¹ç›®æŒ‡å—**: [ICEBERG_GUIDE.md](ICEBERG_GUIDE.md)
- **ç¤ºä¾‹ä»£ç **: `src/batch/ingest_iceberg.py` å’Œ `src/batch/iceberg_features.py`
- **æµ‹è¯•ç”¨ä¾‹**: `tests/test_iceberg.py`

---

**å¼€å§‹ä½ çš„ Iceberg ä¹‹æ—…å§ï¼ğŸš€**

æœ‰é—®é¢˜ï¼ŸæŸ¥çœ‹ [ICEBERG_GUIDE.md](ICEBERG_GUIDE.md) æˆ–æ Issueï¼

