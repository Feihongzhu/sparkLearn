# Apache Iceberg å­¦ä¹ æŒ‡å—

## ğŸ“š ä»€ä¹ˆæ˜¯ Apache Icebergï¼Ÿ

Apache Iceberg æ˜¯ä¸€ä¸ªå¼€æºçš„è¡¨æ ¼å¼ï¼ˆtable formatï¼‰ï¼Œä¸“ä¸ºå¤§è§„æ¨¡åˆ†æå‹æ•°æ®æ¹–è®¾è®¡ã€‚å®ƒæ˜¯ä¸€ä¸ª**æ•°æ®è¡¨çš„å…ƒæ•°æ®å±‚**ï¼Œä½äºå­˜å‚¨æ ¼å¼ï¼ˆå¦‚ Parquetã€ORCï¼‰ä¹‹ä¸Šã€‚

### æ ¸å¿ƒæ¦‚å¿µ

```
åº”ç”¨å±‚ (Spark, Flink, Trino)
       â†“
è¡¨æ ¼å¼å±‚ (Iceberg)  â† å…ƒæ•°æ®ç®¡ç†ã€äº‹åŠ¡ã€ç‰ˆæœ¬æ§åˆ¶
       â†“
å­˜å‚¨æ ¼å¼ (Parquet, ORC)
       â†“
å­˜å‚¨ç³»ç»Ÿ (HDFS, S3, æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ)
```

## ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨ Icebergï¼Ÿ

### 1. **ACID äº‹åŠ¡**
- ä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼Œæ”¯æŒå¹¶å‘è¯»å†™
- é¿å…è¯»å–åˆ°ä¸å®Œæ•´çš„æ•°æ®

### 2. **Schema Evolutionï¼ˆæ¨¡å¼æ¼”å˜ï¼‰**
- æ·»åŠ ã€åˆ é™¤ã€é‡å‘½ååˆ—
- å‘åå…¼å®¹ï¼Œæ— éœ€é‡å†™æ‰€æœ‰æ•°æ®

### 3. **Time Travelï¼ˆæ—¶é—´æ—…è¡Œï¼‰**
- æŸ¥è¯¢ä»»æ„å†å²ç‰ˆæœ¬çš„æ•°æ®
- ç”¨äºæ•°æ®å®¡è®¡ã€å›æ»šã€å¯¹æ¯”åˆ†æ

### 4. **éšè—åˆ†åŒºï¼ˆHidden Partitioningï¼‰**
- ç”¨æˆ·æŸ¥è¯¢æ—¶ä¸éœ€è¦çŸ¥é“åˆ†åŒºç»†èŠ‚
- è‡ªåŠ¨åˆ†åŒºè£å‰ªï¼Œæå‡æŸ¥è¯¢æ€§èƒ½

### 5. **å¢é‡è¯»å–**
- åªè¯»å–å˜æ›´çš„æ•°æ®ï¼Œæå‡ ETL æ•ˆç‡
- æ”¯æŒæµå¼å¤„ç†åœºæ™¯

### 6. **æ€§èƒ½ä¼˜åŒ–**
- æ•°æ®è·³è¿‡ï¼ˆData Skippingï¼‰
- å°æ–‡ä»¶åˆå¹¶ï¼ˆCompactionï¼‰
- Z-Order æ’åºä¼˜åŒ–

## ğŸ—ï¸ Iceberg æ¶æ„

### å…ƒæ•°æ®å±‚æ¬¡ç»“æ„

```
Catalog (ç›®å½•)
  â†“
Table (è¡¨)
  â†“
Metadata File (å…ƒæ•°æ®æ–‡ä»¶) - æŒ‡å‘å¿«ç…§
  â†“
Snapshot (å¿«ç…§) - è¡¨åœ¨æŸä¸ªæ—¶é—´ç‚¹çš„çŠ¶æ€
  â†“
Manifest List (æ¸…å•åˆ—è¡¨) - æŒ‡å‘å¤šä¸ª manifest
  â†“
Manifest File (æ¸…å•æ–‡ä»¶) - åŒ…å«æ•°æ®æ–‡ä»¶åˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
  â†“
Data Files (æ•°æ®æ–‡ä»¶) - å®é™…çš„ Parquet/ORC æ–‡ä»¶
```

### å¿«ç…§ï¼ˆSnapshotï¼‰æœºåˆ¶

æ¯æ¬¡å†™æ“ä½œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°å¿«ç…§ï¼š
- **å¿«ç…§ ID**: å”¯ä¸€æ ‡è¯†ä¸€ä¸ªç‰ˆæœ¬
- **å¿«ç…§æ—¶é—´**: åˆ›å»ºæ—¶é—´
- **æ“ä½œç±»å‹**: append, overwrite, delete ç­‰
- **æ¸…å•åˆ—è¡¨**: æŒ‡å‘æ•°æ®æ–‡ä»¶çš„å…ƒæ•°æ®

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### 1. ä¾èµ–å®‰è£…

```bash
# ä½¿ç”¨ pip å®‰è£…ï¼ˆPythonï¼‰
pip install pyspark==3.5.0

# ä¸‹è½½ Iceberg Spark Runtime JARï¼ˆæ ¹æ® Spark ç‰ˆæœ¬é€‰æ‹©ï¼‰
# Spark 3.5: iceberg-spark-runtime-3.5_2.12:1.4.3
```

### 2. Spark é…ç½®

åœ¨ `spark_session.py` ä¸­é…ç½®ï¼š

```python
builder = (
    SparkSession.builder
    # Iceberg æ‰©å±•
    .config("spark.sql.extensions", 
            "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    # Catalog é…ç½®
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.local.type", "hadoop")
    .config("spark.sql.catalog.local.warehouse", "data/iceberg-warehouse")
)
```

### 3. Catalog ç±»å‹é€‰æ‹©

| Catalog ç±»å‹ | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ |
|-------------|---------|------|
| Hadoop      | æœ¬åœ°/HDFS | ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨å…ƒæ•°æ® |
| Hive        | å·²æœ‰ Hive ç¯å¢ƒ | å…¼å®¹ Hive Metastore |
| AWS Glue    | AWS äº‘ç¯å¢ƒ | æ‰˜ç®¡å¼å…ƒæ•°æ®æœåŠ¡ |
| REST        | å¤šå¼•æ“å…±äº« | é€šè¿‡ REST API è®¿é—® |

## ğŸ“– åŸºæœ¬ä½¿ç”¨

### 1. åˆ›å»º Iceberg è¡¨

**æ–¹å¼ä¸€ï¼šé€šè¿‡ SQL**

```python
spark.sql("""
    CREATE TABLE local.db.orders (
        order_id STRING,
        user_id STRING,
        amount DOUBLE,
        ts TIMESTAMP
    )
    USING iceberg
    PARTITIONED BY (days(ts))
""")
```

**æ–¹å¼äºŒï¼šé€šè¿‡ DataFrame**

```python
df.writeTo("local.db.orders") \
  .using("iceberg") \
  .partitionedBy("order_date") \
  .create()
```

### 2. å†™å…¥æ•°æ®

```python
# Append æ¨¡å¼
df.writeTo("local.db.orders").using("iceberg").append()

# Overwrite æ¨¡å¼
df.writeTo("local.db.orders").using("iceberg").overwrite()

# ä¹Ÿå¯ä»¥ä½¿ç”¨ä¼ ç»Ÿ write API
df.write.format("iceberg").mode("append").save("local.db.orders")
```

### 3. è¯»å–æ•°æ®

```python
# è¯»å–å½“å‰ç‰ˆæœ¬
df = spark.table("local.db.orders")

# æˆ–ä½¿ç”¨ read API
df = spark.read.format("iceberg").load("local.db.orders")
```

### 4. Time Travel

```python
# ä½¿ç”¨å¿«ç…§ ID
df = spark.read \
    .option("snapshot-id", 1234567890) \
    .format("iceberg") \
    .load("local.db.orders")

# ä½¿ç”¨æ—¶é—´æˆ³
df = spark.read \
    .option("as-of-timestamp", "1696636800000") \
    .format("iceberg") \
    .load("local.db.orders")

# ä½¿ç”¨ SQL
spark.sql("""
    SELECT * FROM local.db.orders
    VERSION AS OF 1234567890
""")
```

### 5. Schema Evolution

```python
# æ·»åŠ åˆ—
spark.sql("ALTER TABLE local.db.orders ADD COLUMN status STRING")

# åˆ é™¤åˆ—
spark.sql("ALTER TABLE local.db.orders DROP COLUMN status")

# é‡å‘½ååˆ—
spark.sql("ALTER TABLE local.db.orders RENAME COLUMN amount TO total_amount")
```

### 6. æ›´æ–°å’Œåˆ é™¤

```python
# UPDATE
spark.sql("""
    UPDATE local.db.orders
    SET status = 'completed'
    WHERE order_id = '12345'
""")

# DELETE
spark.sql("""
    DELETE FROM local.db.orders
    WHERE ts < '2025-01-01'
""")
```

### 7. MERGE INTO (UPSERT)

```python
spark.sql("""
    MERGE INTO local.db.orders t
    USING updates u
    ON t.order_id = u.order_id
    WHEN MATCHED THEN
        UPDATE SET t.amount = u.amount, t.ts = u.ts
    WHEN NOT MATCHED THEN
        INSERT *
""")
```

### 8. å¢é‡è¯»å–

```python
# è¯»å–ä¸¤ä¸ªå¿«ç…§ä¹‹é—´çš„å¢é‡æ•°æ®
df = spark.read \
    .format("iceberg") \
    .option("start-snapshot-id", start_id) \
    .option("end-snapshot-id", end_id) \
    .load("local.db.orders")
```

## ğŸ” å…ƒæ•°æ®æŸ¥è¯¢

Iceberg æä¾›äº†ä¸°å¯Œçš„å…ƒæ•°æ®è¡¨ï¼š

### 1. å¿«ç…§å†å²

```sql
SELECT * FROM local.db.orders.snapshots
```

å­—æ®µï¼š
- `committed_at`: å¿«ç…§åˆ›å»ºæ—¶é—´
- `snapshot_id`: å¿«ç…§ ID
- `parent_id`: çˆ¶å¿«ç…§ ID
- `operation`: æ“ä½œç±»å‹ï¼ˆappend, overwrite, deleteï¼‰

### 2. æ•°æ®æ–‡ä»¶

```sql
SELECT 
    file_path, 
    record_count, 
    file_size_in_bytes 
FROM local.db.orders.files
```

### 3. å†å²è®°å½•

```sql
SELECT * FROM local.db.orders.history
```

### 4. åˆ†åŒºä¿¡æ¯

```sql
SELECT * FROM local.db.orders.partitions
```

## ğŸš€ é«˜çº§ç‰¹æ€§

### 1. è¡¨ç»´æŠ¤æ“ä½œ

**åˆå¹¶å°æ–‡ä»¶ï¼ˆCompactionï¼‰**

```python
spark.sql("""
    CALL local.system.rewrite_data_files(
        table => 'db.orders',
        options => map('target-file-size-bytes', '536870912')
    )
""")
```

**æ¸…ç†è¿‡æœŸå¿«ç…§**

```python
spark.sql("""
    CALL local.system.expire_snapshots(
        table => 'db.orders',
        older_than => TIMESTAMP '2025-10-01 00:00:00',
        retain_last => 5
    )
""")
```

**åˆ é™¤å­¤å„¿æ–‡ä»¶**

```python
spark.sql("""
    CALL local.system.remove_orphan_files(
        table => 'db.orders',
        older_than => TIMESTAMP '2025-09-01 00:00:00'
    )
""")
```

### 2. åˆ†åŒºæ¼”å˜

Iceberg æ”¯æŒä¿®æ”¹åˆ†åŒºç­–ç•¥è€Œæ— éœ€é‡å†™æ•°æ®ï¼š

```sql
ALTER TABLE local.db.orders 
REPLACE PARTITION FIELD days(ts) 
WITH months(ts)
```

### 3. æ’åºä¼˜åŒ–

```python
spark.sql("""
    CALL local.system.rewrite_data_files(
        table => 'db.orders',
        strategy => 'sort',
        sort_order => 'user_id,ts'
    )
""")
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. é€‰æ‹©åˆé€‚çš„åˆ†åŒºç­–ç•¥
- **æ—¶é—´åˆ†åŒº**: é€‚åˆæ—¶åºæ•°æ®ï¼Œä½¿ç”¨ `days(ts)` æˆ– `months(ts)`
- **èŒƒå›´åˆ†åŒº**: é€‚åˆæœ‰æ˜ç¡®èŒƒå›´çš„æ•°æ®ï¼Œå¦‚ `bucket(10, user_id)`

### 2. å®šæœŸåˆå¹¶å°æ–‡ä»¶
- å°æ–‡ä»¶è¿‡å¤šä¼šå½±å“æŸ¥è¯¢æ€§èƒ½
- å»ºè®®å®šæœŸè¿è¡Œ `rewrite_data_files`

### 3. æ¸…ç†è¿‡æœŸå¿«ç…§
- è¿‡å¤šå¿«ç…§ä¼šå ç”¨å­˜å‚¨ç©ºé—´
- ä¿ç•™æœ€è¿‘çš„ N ä¸ªå¿«ç…§å³å¯

### 4. ä½¿ç”¨æ•°æ®è·³è¿‡
- Iceberg è‡ªåŠ¨æ”¶é›†åˆ—çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆmin, max, null countï¼‰
- æŸ¥è¯¢æ—¶è‡ªåŠ¨è·³è¿‡ä¸ç›¸å…³çš„æ–‡ä»¶

## ğŸ†š Iceberg vs Delta Lake vs Hudi

| ç‰¹æ€§ | Iceberg | Delta Lake | Hudi |
|-----|---------|------------|------|
| **å¼€æº** | âœ… Apache | âœ… Linux Foundation | âœ… Apache |
| **ACID** | âœ… | âœ… | âœ… |
| **Time Travel** | âœ… | âœ… | âœ… |
| **Schema Evolution** | âœ… æ›´çµæ´» | âœ… | âœ… |
| **éšè—åˆ†åŒº** | âœ… | âŒ | âŒ |
| **åˆ†åŒºæ¼”å˜** | âœ… | âŒ | âŒ |
| **å¼•æ“æ”¯æŒ** | Spark, Flink, Trino, Hive | ä¸»è¦ Spark | Spark, Flink, Hive |
| **AWS é›†æˆ** | âœ… Glue | âœ… | âœ… |
| **æˆç†Ÿåº¦** | é«˜ | é«˜ | ä¸­ |
| **ç¤¾åŒº** | Netflix, Apple | Databricks | Uber |

### é€‰æ‹©å»ºè®®

- **Iceberg**: éœ€è¦å¤šå¼•æ“æ”¯æŒã€å¤æ‚åˆ†åŒºéœ€æ±‚
- **Delta Lake**: æ·±åº¦ä½¿ç”¨ Databricks å¹³å°
- **Hudi**: CDC åœºæ™¯ã€æ›´æ–°é¢‘ç¹çš„æ•°æ®

## ğŸ§ª å®è·µç»ƒä¹ 

### ç»ƒä¹  1: åŸºç¡€ CRUD æ“ä½œ
1. åˆ›å»º Iceberg è¡¨
2. æ’å…¥æ•°æ®
3. æŸ¥è¯¢æ•°æ®
4. æ›´æ–°æ•°æ®
5. åˆ é™¤æ•°æ®

### ç»ƒä¹  2: Time Travel
1. å¤šæ¬¡å†™å…¥æ•°æ®
2. æŸ¥çœ‹å¿«ç…§å†å²
3. å›æ»šåˆ°å†å²ç‰ˆæœ¬
4. å¯¹æ¯”ä¸åŒç‰ˆæœ¬çš„æ•°æ®

### ç»ƒä¹  3: Schema Evolution
1. æ·»åŠ æ–°åˆ—
2. ä¿®æ”¹åˆ—ç±»å‹
3. é‡å‘½ååˆ—
4. éªŒè¯å†å²æ•°æ®ä»å¯è®¿é—®

### ç»ƒä¹  4: MERGE INTO
1. å®ç° CDC åœºæ™¯
2. å¤„ç†æ›´æ–°å’Œæ’å…¥æ··åˆæ“ä½œ
3. éªŒè¯æ•°æ®ä¸€è‡´æ€§

### ç»ƒä¹  5: å¢é‡ ETL
1. ä½¿ç”¨å¢é‡è¯»å–æ„å»º ETL ç®¡é“
2. åªå¤„ç†æ–°å¢æ•°æ®
3. è¿½è¸ªå·²å¤„ç†çš„å¿«ç…§ ID

## ğŸ“š å‚è€ƒèµ„æº

- [Iceberg å®˜æ–¹æ–‡æ¡£](https://iceberg.apache.org/)
- [Iceberg GitHub](https://github.com/apache/iceberg)
- [Iceberg Spark é›†æˆ](https://iceberg.apache.org/docs/latest/spark-ddl/)
- [Iceberg æ¶æ„è®¾è®¡](https://iceberg.apache.org/docs/latest/spec/)

## ğŸ“ è¿›é˜¶å­¦ä¹ è·¯å¾„

1. **Week 1-2**: åŸºç¡€æ¦‚å¿µå’Œ CRUD æ“ä½œ
2. **Week 3**: Time Travel å’Œ Schema Evolution
3. **Week 4**: MERGE INTO å’Œå¢é‡è¯»å–
4. **Week 5**: è¡¨ç»´æŠ¤å’Œæ€§èƒ½ä¼˜åŒ–
5. **Week 6**: ä¸å…¶ä»–ç»„ä»¶é›†æˆï¼ˆFlink, Trinoï¼‰
6. **Week 7**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ˆAWS Glue, S3ï¼‰
7. **Week 8**: æ•…éšœæ¢å¤å’Œç›‘æ§

## âš ï¸ å¸¸è§é—®é¢˜

### 1. Iceberg JAR ç‰ˆæœ¬ä¸åŒ¹é…
**é—®é¢˜**: `ClassNotFoundException` æˆ– `NoSuchMethodError`

**è§£å†³**: ç¡®ä¿ Iceberg JAR ç‰ˆæœ¬ä¸ Spark ç‰ˆæœ¬åŒ¹é…
- Spark 3.3 â†’ iceberg-spark-runtime-3.3_2.12
- Spark 3.4 â†’ iceberg-spark-runtime-3.4_2.12
- Spark 3.5 â†’ iceberg-spark-runtime-3.5_2.12

### 2. å†™å…¥æƒé™é—®é¢˜
**é—®é¢˜**: `PermissionDeniedException`

**è§£å†³**: æ£€æŸ¥ warehouse ç›®å½•æƒé™

### 3. å¿«ç…§è¿‡å¤š
**é—®é¢˜**: å­˜å‚¨ç©ºé—´å ç”¨å¤§

**è§£å†³**: å®šæœŸæ‰§è¡Œ `expire_snapshots`

### 4. å°æ–‡ä»¶è¿‡å¤š
**é—®é¢˜**: æŸ¥è¯¢æ…¢

**è§£å†³**: å®šæœŸæ‰§è¡Œ `rewrite_data_files`

---

**Happy Learning! ğŸš€**

