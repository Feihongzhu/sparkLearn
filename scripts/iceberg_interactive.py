#!/usr/bin/env python3
"""
Iceberg äº¤äº’å¼æ¼”ç¤ºè„šæœ¬

åœ¨ Python REPL ä¸­å¿«é€Ÿä½“éªŒ Iceberg åŠŸèƒ½
"""
from pyspark.sql import SparkSession
from pyspark.sql import functions as F


def create_spark_session():
    """åˆ›å»ºå¸¦ Iceberg æ”¯æŒçš„ Spark Session"""
    print("ğŸš€ æ­£åœ¨åˆ›å»º Spark Session...")
    spark = (
        SparkSession.builder
        .appName("IcebergInteractive")
        .master("local[*]")
        .config("spark.sql.extensions", 
                "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
        .config("spark.sql.catalog.local.type", "hadoop")
        .config("spark.sql.catalog.local.warehouse", "data/iceberg-warehouse")
        .config("spark.sql.defaultCatalog", "local")
        .getOrCreate()
    )
    print("âœ… Spark Session åˆ›å»ºæˆåŠŸï¼")
    return spark


def setup_demo_table(spark):
    """åˆ›å»ºç¤ºä¾‹è¡¨å’Œæ•°æ®"""
    print("\nğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    # åˆ›å»ºæ•°æ®åº“
    spark.sql("CREATE DATABASE IF NOT EXISTS local.demo")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    data = [
        ("order_001", "user_A", "laptop", 999.99, 1, "2025-11-01 10:00:00"),
        ("order_002", "user_B", "mouse", 29.99, 2, "2025-11-01 11:00:00"),
        ("order_003", "user_A", "keyboard", 79.99, 1, "2025-11-02 09:00:00"),
        ("order_004", "user_C", "monitor", 299.99, 2, "2025-11-02 14:00:00"),
        ("order_005", "user_B", "laptop", 1299.99, 1, "2025-11-03 10:30:00"),
    ]
    
    df = spark.createDataFrame(
        data, 
        ["order_id", "user_id", "product", "price", "quantity", "order_time"]
    )
    
    df = df.withColumn("order_time", F.to_timestamp("order_time")) \
           .withColumn("amount", F.col("price") * F.col("quantity")) \
           .withColumn("order_date", F.to_date("order_time"))
    
    # åˆ é™¤æ—§è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    spark.sql("DROP TABLE IF EXISTS local.demo.orders")
    
    # åˆ›å»º Iceberg è¡¨
    df.writeTo("local.demo.orders") \
      .using("iceberg") \
      .partitionedBy("order_date") \
      .create()
    
    print("âœ… ç¤ºä¾‹è¡¨åˆ›å»ºæˆåŠŸï¼")
    print("\nğŸ“‹ è¡¨å†…å®¹ï¼š")
    spark.table("local.demo.orders").show()
    
    return spark


def print_menu():
    """æ‰“å°èœå•"""
    print("\n" + "="*60)
    print("ğŸ§Š Iceberg äº¤äº’å¼æ¼”ç¤º")
    print("="*60)
    print("1. æŸ¥çœ‹è¡¨æ•°æ®")
    print("2. æŸ¥çœ‹å¿«ç…§å†å²")
    print("3. æŸ¥çœ‹æ•°æ®æ–‡ä»¶")
    print("4. è¿½åŠ æ•°æ®")
    print("5. Time Travelï¼ˆæŸ¥è¯¢å†å²ç‰ˆæœ¬ï¼‰")
    print("6. Schema Evolutionï¼ˆæ·»åŠ åˆ—ï¼‰")
    print("7. UPDATE æ•°æ®")
    print("8. DELETE æ•°æ®")
    print("9. MERGE INTO æ¼”ç¤º")
    print("10. å¢é‡è¯»å–")
    print("11. è¡¨ç»´æŠ¤ï¼ˆåˆå¹¶å°æ–‡ä»¶ï¼‰")
    print("0. é€€å‡º")
    print("="*60)


def demo_query(spark):
    """æŸ¥çœ‹è¡¨æ•°æ®"""
    print("\nğŸ“Š å½“å‰è¡¨æ•°æ®ï¼š")
    spark.table("local.demo.orders").show()
    print(f"æ€»è®°å½•æ•°: {spark.table('local.demo.orders').count()}")


def demo_snapshots(spark):
    """æŸ¥çœ‹å¿«ç…§å†å²"""
    print("\nğŸ“¸ å¿«ç…§å†å²ï¼š")
    spark.sql("SELECT snapshot_id, committed_at, operation FROM local.demo.orders.snapshots").show(truncate=False)


def demo_files(spark):
    """æŸ¥çœ‹æ•°æ®æ–‡ä»¶"""
    print("\nğŸ“ æ•°æ®æ–‡ä»¶ï¼š")
    spark.sql("""
        SELECT 
            file_path, 
            record_count, 
            ROUND(file_size_in_bytes / 1024, 2) as size_kb
        FROM local.demo.orders.files
    """).show(truncate=False)


def demo_append(spark):
    """è¿½åŠ æ•°æ®"""
    print("\nâ• è¿½åŠ æ–°æ•°æ®...")
    new_data = [
        ("order_006", "user_D", "headphone", 199.99, 1, "2025-11-04 15:00:00"),
    ]
    
    new_df = spark.createDataFrame(
        new_data,
        ["order_id", "user_id", "product", "price", "quantity", "order_time"]
    )
    
    new_df = new_df.withColumn("order_time", F.to_timestamp("order_time")) \
                   .withColumn("amount", F.col("price") * F.col("quantity")) \
                   .withColumn("order_date", F.to_date("order_time"))
    
    new_df.writeTo("local.demo.orders").using("iceberg").append()
    
    print("âœ… æ•°æ®è¿½åŠ æˆåŠŸï¼")
    demo_query(spark)


def demo_time_travel(spark):
    """Time Travel æ¼”ç¤º"""
    print("\nğŸ• Time Travel æ¼”ç¤ºï¼š")
    
    snapshots = spark.sql("SELECT snapshot_id FROM local.demo.orders.snapshots ORDER BY committed_at")
    snapshot_list = snapshots.collect()
    
    if len(snapshot_list) > 0:
        first_snapshot = snapshot_list[0][0]
        print(f"\næŸ¥è¯¢ç¬¬ä¸€ä¸ªå¿«ç…§ (ID: {first_snapshot}):")
        
        df_history = spark.read \
            .option("snapshot-id", first_snapshot) \
            .format("iceberg") \
            .load("local.demo.orders")
        
        df_history.show()
        print(f"å†å²å¿«ç…§è®°å½•æ•°: {df_history.count()}")
    else:
        print("âš ï¸ æš‚æ— å¿«ç…§å†å²")


def demo_schema_evolution(spark):
    """Schema Evolution æ¼”ç¤º"""
    print("\nğŸ“ Schema Evolution æ¼”ç¤ºï¼š")
    print("æ·»åŠ æ–°åˆ— 'discount'...")
    
    try:
        spark.sql("ALTER TABLE local.demo.orders ADD COLUMN discount DOUBLE")
        print("âœ… åˆ—æ·»åŠ æˆåŠŸï¼")
        
        spark.sql("""
            UPDATE local.demo.orders
            SET discount = CASE 
                WHEN amount > 1000 THEN 0.1
                WHEN amount > 500 THEN 0.05
                ELSE 0.0
            END
            WHERE discount IS NULL
        """)
        
        print("\næ›´æ–°åçš„æ•°æ®ï¼š")
        spark.sql("SELECT order_id, amount, discount FROM local.demo.orders LIMIT 5").show()
    except Exception as e:
        if "already exists" in str(e):
            print("âš ï¸ åˆ— 'discount' å·²å­˜åœ¨")
        else:
            print(f"âŒ é”™è¯¯: {e}")


def demo_update(spark):
    """UPDATE æ¼”ç¤º"""
    print("\nâœï¸ UPDATE æ¼”ç¤ºï¼š")
    print("å°† user_A çš„æ‰€æœ‰è®¢å•é‡‘é¢å¢åŠ  10%...")
    
    spark.sql("""
        UPDATE local.demo.orders
        SET amount = amount * 1.1
        WHERE user_id = 'user_A'
    """)
    
    print("âœ… æ›´æ–°å®Œæˆï¼")
    print("\nuser_A çš„è®¢å•ï¼š")
    spark.sql("SELECT * FROM local.demo.orders WHERE user_id = 'user_A'").show()


def demo_delete(spark):
    """DELETE æ¼”ç¤º"""
    print("\nğŸ—‘ï¸ DELETE æ¼”ç¤ºï¼š")
    print("åˆ é™¤é‡‘é¢å°äº 50 çš„è®¢å•...")
    
    before_count = spark.table("local.demo.orders").count()
    
    spark.sql("DELETE FROM local.demo.orders WHERE amount < 50")
    
    after_count = spark.table("local.demo.orders").count()
    
    print(f"âœ… åˆ é™¤å®Œæˆï¼åˆ é™¤äº† {before_count - after_count} æ¡è®°å½•")
    demo_query(spark)


def demo_merge(spark):
    """MERGE INTO æ¼”ç¤º"""
    print("\nğŸ”„ MERGE INTO æ¼”ç¤ºï¼š")
    
    # åˆ›å»ºæ›´æ–°æ•°æ®
    updates_data = [
        ("order_001", "user_A", "laptop", 899.99, 1, "2025-11-01 10:00:00"),  # æ›´æ–°ä»·æ ¼
        ("order_999", "user_E", "tablet", 499.99, 1, "2025-11-05 09:00:00"),  # æ–°è®¢å•
    ]
    
    updates_df = spark.createDataFrame(
        updates_data,
        ["order_id", "user_id", "product", "price", "quantity", "order_time"]
    )
    
    updates_df = updates_df.withColumn("order_time", F.to_timestamp("order_time")) \
                           .withColumn("amount", F.col("price") * F.col("quantity")) \
                           .withColumn("order_date", F.to_date("order_time"))
    
    updates_df.createOrReplaceTempView("updates")
    
    print("æ›´æ–°æ•°æ®ï¼š")
    updates_df.show()
    
    spark.sql("""
        MERGE INTO local.demo.orders t
        USING updates u
        ON t.order_id = u.order_id
        WHEN MATCHED THEN
            UPDATE SET 
                t.price = u.price,
                t.amount = u.amount
        WHEN NOT MATCHED THEN
            INSERT *
    """)
    
    print("âœ… MERGE å®Œæˆï¼")
    demo_query(spark)


def demo_incremental(spark):
    """å¢é‡è¯»å–æ¼”ç¤º"""
    print("\nğŸ“– å¢é‡è¯»å–æ¼”ç¤ºï¼š")
    
    snapshots = spark.sql("SELECT snapshot_id FROM local.demo.orders.snapshots ORDER BY committed_at")
    snapshot_list = snapshots.collect()
    
    if len(snapshot_list) >= 2:
        start_snapshot = snapshot_list[0][0]
        end_snapshot = snapshot_list[-1][0]
        
        print(f"è¯»å–ä»å¿«ç…§ {start_snapshot} åˆ° {end_snapshot} çš„å¢é‡æ•°æ®ï¼š")
        
        incremental_df = spark.read \
            .format("iceberg") \
            .option("start-snapshot-id", start_snapshot) \
            .option("end-snapshot-id", end_snapshot) \
            .load("local.demo.orders")
        
        print(f"å¢é‡è®°å½•æ•°: {incremental_df.count()}")
        incremental_df.show()
    else:
        print("âš ï¸ å¿«ç…§æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦ 2 ä¸ªï¼‰")


def demo_maintenance(spark):
    """è¡¨ç»´æŠ¤æ¼”ç¤º"""
    print("\nğŸ”§ è¡¨ç»´æŠ¤æ¼”ç¤ºï¼š")
    
    print("1ï¸âƒ£ å½“å‰æ–‡ä»¶çŠ¶æ€ï¼š")
    demo_files(spark)
    
    print("\n2ï¸âƒ£ æ‰§è¡Œæ–‡ä»¶åˆå¹¶...")
    try:
        spark.sql("CALL local.system.rewrite_data_files(table => 'demo.orders')")
        print("âœ… æ–‡ä»¶åˆå¹¶å®Œæˆï¼")
        
        print("\nä¼˜åŒ–åçš„æ–‡ä»¶çŠ¶æ€ï¼š")
        demo_files(spark)
    except Exception as e:
        print(f"âš ï¸ ç»´æŠ¤æ“ä½œå¤±è´¥: {e}")
        print("æç¤º: å°è§„æ¨¡æ•°æ®å¯èƒ½ä¸éœ€è¦æ–‡ä»¶åˆå¹¶")


def main():
    """ä¸»å‡½æ•°"""
    spark = create_spark_session()
    setup_demo_table(spark)
    
    actions = {
        "1": demo_query,
        "2": demo_snapshots,
        "3": demo_files,
        "4": demo_append,
        "5": demo_time_travel,
        "6": demo_schema_evolution,
        "7": demo_update,
        "8": demo_delete,
        "9": demo_merge,
        "10": demo_incremental,
        "11": demo_maintenance,
    }
    
    while True:
        print_menu()
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-11): ").strip()
        
        if choice == "0":
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ Iceberg äº¤äº’å¼æ¼”ç¤ºï¼")
            break
        
        if choice in actions:
            try:
                actions[choice](spark)
            except Exception as e:
                print(f"\nâŒ æ“ä½œå¤±è´¥: {e}")
            
            input("\næŒ‰ Enter ç»§ç»­...")
        else:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•")
    
    spark.stop()


if __name__ == "__main__":
    main()

