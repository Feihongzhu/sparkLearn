# ğŸ§Š Iceberg é€ŸæŸ¥è¡¨ (Cheat Sheet)

å¿«é€Ÿå‚è€ƒ Apache Iceberg çš„å¸¸ç”¨å‘½ä»¤å’Œä»£ç ç‰‡æ®µã€‚

---

## ğŸ“¦ ç¯å¢ƒè®¾ç½®

### Spark Session é…ç½®

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("IcebergApp")
    .master("local[*]")
    # Iceberg æ‰©å±•
    .config("spark.sql.extensions", 
            "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    # Catalog é…ç½®
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.local.type", "hadoop")
    .config("spark.sql.catalog.local.warehouse", "data/iceberg-warehouse")
    .getOrCreate()
)
```

---

## ğŸ—‚ï¸ åŸºæœ¬ DDL æ“ä½œ

### åˆ›å»ºæ•°æ®åº“

```sql
CREATE DATABASE IF NOT EXISTS local.my_db;
```

### åˆ›å»ºè¡¨

```sql
-- SQL æ–¹å¼
CREATE TABLE local.my_db.orders (
    order_id STRING,
    user_id STRING,
    amount DOUBLE,
    order_date DATE
)
USING iceberg
PARTITIONED BY (order_date);
```

```python
# DataFrame æ–¹å¼
df.writeTo("local.my_db.orders") \
  .using("iceberg") \
  .partitionedBy("order_date") \
  .create()
```

### æŸ¥çœ‹è¡¨

```sql
SHOW TABLES IN local.my_db;
DESCRIBE local.my_db.orders;
DESCRIBE EXTENDED local.my_db.orders;
```

### åˆ é™¤è¡¨

```sql
DROP TABLE local.my_db.orders;
```

---

## ğŸ“ æ•°æ®æ“ä½œ (DML)

### æ’å…¥æ•°æ®

```sql
INSERT INTO local.my_db.orders 
VALUES ('order_001', 'user_A', 100.0, '2025-11-01');
```

```python
# DataFrame æ–¹å¼
df.writeTo("local.my_db.orders").using("iceberg").append()
```

### æŸ¥è¯¢æ•°æ®

```sql
SELECT * FROM local.my_db.orders WHERE order_date = '2025-11-01';
```

```python
df = spark.table("local.my_db.orders")
df.show()
```

### æ›´æ–°æ•°æ®

```sql
UPDATE local.my_db.orders
SET amount = 200.0
WHERE order_id = 'order_001';
```

### åˆ é™¤æ•°æ®

```sql
DELETE FROM local.my_db.orders
WHERE order_date < '2025-01-01';
```

---

## ğŸ”„ MERGE INTO (UPSERT)

```sql
MERGE INTO local.my_db.orders t
USING updates u
ON t.order_id = u.order_id
WHEN MATCHED THEN
    UPDATE SET t.amount = u.amount
WHEN NOT MATCHED THEN
    INSERT *
```

```python
# Python ç¤ºä¾‹
updates_df.createOrReplaceTempView("updates")

spark.sql("""
    MERGE INTO local.my_db.orders t
    USING updates u
    ON t.order_id = u.order_id
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
""")
```

---

## ğŸ• Time Travel

### ä½¿ç”¨å¿«ç…§ ID

```sql
SELECT * FROM local.my_db.orders
VERSION AS OF 1234567890;
```

```python
df = spark.read \
    .option("snapshot-id", 1234567890) \
    .format("iceberg") \
    .load("local.my_db.orders")
```

### ä½¿ç”¨æ—¶é—´æˆ³

```sql
SELECT * FROM local.my_db.orders
TIMESTAMP AS OF '2025-11-01 10:00:00';
```

```python
df = spark.read \
    .option("as-of-timestamp", "1696636800000") \
    .format("iceberg") \
    .load("local.my_db.orders")
```

---

## ğŸ“‹ Schema Evolution

### æ·»åŠ åˆ—

```sql
ALTER TABLE local.my_db.orders 
ADD COLUMN status STRING;
```

### åˆ é™¤åˆ—

```sql
ALTER TABLE local.my_db.orders 
DROP COLUMN status;
```

### é‡å‘½ååˆ—

```sql
ALTER TABLE local.my_db.orders 
RENAME COLUMN amount TO total_amount;
```

### ä¿®æ”¹åˆ—ç±»å‹

```sql
ALTER TABLE local.my_db.orders 
ALTER COLUMN amount TYPE DECIMAL(10, 2);
```

---

## ğŸ“– å¢é‡è¯»å–

### è¯»å–ä¸¤ä¸ªå¿«ç…§ä¹‹é—´çš„æ•°æ®

```python
df = spark.read \
    .format("iceberg") \
    .option("start-snapshot-id", start_id) \
    .option("end-snapshot-id", end_id) \
    .load("local.my_db.orders")
```

### ä»æŸä¸ªå¿«ç…§å¼€å§‹è¯»å–

```python
df = spark.read \
    .format("iceberg") \
    .option("start-snapshot-id", start_id) \
    .load("local.my_db.orders")
```

---

## ğŸ” å…ƒæ•°æ®æŸ¥è¯¢

### æŸ¥çœ‹å¿«ç…§å†å²

```sql
SELECT snapshot_id, committed_at, operation 
FROM local.my_db.orders.snapshots
ORDER BY committed_at DESC;
```

### æŸ¥çœ‹æ•°æ®æ–‡ä»¶

```sql
SELECT file_path, record_count, file_size_in_bytes
FROM local.my_db.orders.files;
```

### æŸ¥çœ‹å†å²è®°å½•

```sql
SELECT * FROM local.my_db.orders.history;
```

### æŸ¥çœ‹åˆ†åŒºä¿¡æ¯

```sql
SELECT * FROM local.my_db.orders.partitions;
```

### æŸ¥çœ‹æ¸…å•ï¼ˆManifestï¼‰

```sql
SELECT * FROM local.my_db.orders.manifests;
```

---

## ğŸ”§ è¡¨ç»´æŠ¤æ“ä½œ

### åˆå¹¶å°æ–‡ä»¶

```sql
CALL local.system.rewrite_data_files(
    table => 'my_db.orders'
);

-- æŒ‡å®šç›®æ ‡æ–‡ä»¶å¤§å°
CALL local.system.rewrite_data_files(
    table => 'my_db.orders',
    options => map('target-file-size-bytes', '536870912')  -- 512MB
);
```

### æ¸…ç†è¿‡æœŸå¿«ç…§

```sql
CALL local.system.expire_snapshots(
    table => 'my_db.orders',
    older_than => TIMESTAMP '2025-10-01 00:00:00',
    retain_last => 5
);
```

### åˆ é™¤å­¤å„¿æ–‡ä»¶

```sql
CALL local.system.remove_orphan_files(
    table => 'my_db.orders',
    older_than => TIMESTAMP '2025-09-01 00:00:00'
);
```

### é‡å†™æ¸…å•æ–‡ä»¶

```sql
CALL local.system.rewrite_manifests('my_db.orders');
```

---

## ğŸ¯ åˆ†åŒºæ“ä½œ

### åˆ›å»ºåˆ†åŒºè¡¨

```sql
CREATE TABLE local.my_db.orders (
    order_id STRING,
    ts TIMESTAMP,
    amount DOUBLE
)
USING iceberg
PARTITIONED BY (days(ts));  -- æŒ‰å¤©åˆ†åŒº
```

### åˆ†åŒºå˜æ¢å‡½æ•°

```sql
-- æŒ‰å¹´åˆ†åŒº
PARTITIONED BY (years(ts))

-- æŒ‰æœˆåˆ†åŒº
PARTITIONED BY (months(ts))

-- æŒ‰å¤©åˆ†åŒº
PARTITIONED BY (days(ts))

-- æŒ‰å°æ—¶åˆ†åŒº
PARTITIONED BY (hours(ts))

-- æ¡¶åˆ†åŒº
PARTITIONED BY (bucket(10, user_id))

-- æˆªæ–­åˆ†åŒº
PARTITIONED BY (truncate(10, user_id))
```

### ä¿®æ”¹åˆ†åŒºç­–ç•¥

```sql
ALTER TABLE local.my_db.orders
REPLACE PARTITION FIELD days(ts) WITH months(ts);
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### è®¾ç½®è¡¨å±æ€§

```sql
ALTER TABLE local.my_db.orders 
SET TBLPROPERTIES (
    'write.format.default' = 'parquet',
    'write.parquet.compression-codec' = 'snappy',
    'write.target-file-size-bytes' = '536870912'  -- 512MB
);
```

### Z-Order æ’åº

```sql
CALL local.system.rewrite_data_files(
    table => 'my_db.orders',
    strategy => 'sort',
    sort_order => 'user_id,order_date'
);
```

---

## ğŸ Python å¸¸ç”¨ä»£ç ç‰‡æ®µ

### è¯»å– Iceberg è¡¨

```python
# æ–¹å¼ 1: ä½¿ç”¨ table
df = spark.table("local.my_db.orders")

# æ–¹å¼ 2: ä½¿ç”¨ read
df = spark.read.format("iceberg").load("local.my_db.orders")
```

### å†™å…¥ Iceberg è¡¨

```python
# Append æ¨¡å¼
df.writeTo("local.my_db.orders").using("iceberg").append()

# Overwrite æ¨¡å¼
df.writeTo("local.my_db.orders").using("iceberg").overwrite()

# ä¼ ç»Ÿ API
df.write.format("iceberg").mode("append").save("local.my_db.orders")
```

### è·å–å¿«ç…§ä¿¡æ¯

```python
snapshots = spark.sql("SELECT * FROM local.my_db.orders.snapshots")
latest_snapshot = snapshots.orderBy("committed_at", ascending=False).first()
snapshot_id = latest_snapshot["snapshot_id"]
```

### æ‰¹é‡æ’å…¥æ•°æ®

```python
from pyspark.sql import functions as F

# åˆ›å»ºæ‰¹é‡æ•°æ®
data = [
    ("order_001", "user_A", 100.0),
    ("order_002", "user_B", 200.0),
]

df = spark.createDataFrame(data, ["order_id", "user_id", "amount"])
df = df.withColumn("order_date", F.current_date())

# è¿½åŠ åˆ° Iceberg è¡¨
df.writeTo("local.my_db.orders").using("iceberg").append()
```

---

## ğŸ› ï¸ Makefile å‘½ä»¤é€ŸæŸ¥

```bash
# ä¸‹è½½ Iceberg JAR
make iceberg-setup

# æœ¬åœ°æ•°æ®æ‘„å…¥
make iceberg-ingest-local

# æœ¬åœ°é«˜çº§ç‰¹æ€§æ¼”ç¤º
make iceberg-features-local

# æœ¬åœ°å®Œæ•´æ¼”ç¤º
make iceberg-demo-local

# äº¤äº’å¼æ¼”ç¤ºï¼ˆæ¨èï¼ï¼‰
make iceberg-interactive

# è¿è¡Œå•å…ƒæµ‹è¯•
make iceberg-test

# æ¸…ç†æ•°æ®
make clean
```

---

## ğŸ“š è¡¨å±æ€§é…ç½®

### å¸¸ç”¨è¡¨å±æ€§

```sql
-- å†™å…¥æ ¼å¼
'write.format.default' = 'parquet'  -- parquet æˆ– orc

-- å‹ç¼©ç¼–ç 
'write.parquet.compression-codec' = 'snappy'  -- snappy, gzip, zstd

-- ç›®æ ‡æ–‡ä»¶å¤§å°
'write.target-file-size-bytes' = '536870912'  -- 512MB

-- å¿«ç…§ä¿ç•™
'history.expire.max-snapshot-age-ms' = '604800000'  -- 7å¤©

-- å…ƒæ•°æ®åˆ·æ–°é—´éš”
'commit.retry.num-retries' = '4'
```

---

## ğŸ” æœ€ä½³å®è·µ

### âœ… DO (æ¨èåšæ³•)

```python
# âœ… ä½¿ç”¨éšè—åˆ†åŒº
CREATE TABLE orders (...) PARTITIONED BY (days(ts))

# âœ… å®šæœŸåˆå¹¶å°æ–‡ä»¶
CALL system.rewrite_data_files('orders')

# âœ… å®šæœŸæ¸…ç†å¿«ç…§
CALL system.expire_snapshots('orders', retain_last => 5)

# âœ… ä½¿ç”¨ writeTo API
df.writeTo("orders").using("iceberg").append()
```

### âŒ DON'T (é¿å…åšæ³•)

```python
# âŒ ä¸è¦åˆ›å»ºè¿‡å¤šå°æ–‡ä»¶
for row in data:
    single_df.writeTo("orders").append()  # æ¯æ¬¡å†™ä¸€è¡Œ

# âŒ ä¸è¦ä¿ç•™è¿‡å¤šå¿«ç…§
# å®šæœŸæ¸…ç†ï¼Œé¿å…å…ƒæ•°æ®è†¨èƒ€

# âŒ ä¸è¦æ‰‹åŠ¨ç®¡ç†åˆ†åŒºè·¯å¾„
df.write.partitionBy("date").parquet("path/date=2025-11-01")  # ä¸æ¨è
```

---

## ğŸš¨ å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### é”™è¯¯ 1: ClassNotFoundException

```
java.lang.ClassNotFoundException: org.apache.iceberg.spark.SparkCatalog
```

**è§£å†³**: ä¸‹è½½å¹¶æ·»åŠ  Iceberg JAR

```bash
make iceberg-setup
```

### é”™è¯¯ 2: è¡¨ä¸å­˜åœ¨

```
org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist
```

**è§£å†³**: å…ˆåˆ›å»ºè¡¨æˆ–ä½¿ç”¨æ­£ç¡®çš„è¡¨å

```sql
CREATE TABLE IF NOT EXISTS local.my_db.orders (...) USING iceberg;
```

### é”™è¯¯ 3: å¿«ç…§ä¸å­˜åœ¨

```
Cannot find snapshot with id
```

**è§£å†³**: æŸ¥çœ‹å¯ç”¨å¿«ç…§

```sql
SELECT snapshot_id FROM orders.snapshots;
```

---

## ğŸ“ æ›´å¤šèµ„æº

- å®Œæ•´æŒ‡å—: `ICEBERG_GUIDE.md`
- å¿«é€Ÿå…¥é—¨: `ICEBERG_QUICKSTART.md`
- æ€»ç»“æ–‡æ¡£: `ICEBERG_SUMMARY.md`
- å®˜æ–¹æ–‡æ¡£: https://iceberg.apache.org/

---

**ä¿å­˜æ­¤æ–‡ä»¶ä»¥ä¾¿å¿«é€ŸæŸ¥è¯¢ï¼ğŸš€**

