"""
Apache Iceberg é«˜çº§ç‰¹æ€§æ¼”ç¤º

åŒ…å«:
1. Time Travel (æ—¶é—´æ—…è¡Œ)
2. Schema Evolution (æ¨¡å¼æ¼”å˜)
3. MERGE INTO (åˆå¹¶æ›´æ–°)
4. å¢é‡è¯»å–
5. è¡¨ç»´æŠ¤æ“ä½œ (OPTIMIZE, EXPIRE SNAPSHOTS)
"""
import argparse
from datetime import datetime

from pyspark.sql import functions as F

from src.common.spark_session import build_spark


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Iceberg advanced features demo")
    parser.add_argument("--table", default="local.db.orders", help="Iceberg table name")
    parser.add_argument("--local", action="store_true", help="Use local[*] Spark")
    parser.add_argument(
        "--feature",
        choices=["time_travel", "schema_evolution", "merge", "incremental", "maintenance", "all"],
        default="all",
        help="é€‰æ‹©è¦æ¼”ç¤ºçš„ç‰¹æ€§"
    )
    return parser.parse_args()


def demo_time_travel(spark, table_name: str):
    """
    æ¼”ç¤º Time Travel åŠŸèƒ½
    
    Iceberg å¯ä»¥æŸ¥è¯¢è¡¨çš„å†å²å¿«ç…§ï¼Œç”¨äº:
    - æ•°æ®å›æ»š
    - å†å²æ•°æ®åˆ†æ
    - å®¡è®¡å’Œåˆè§„
    """
    print("\n" + "="*80)
    print("ğŸ• Time Travel æ¼”ç¤º")
    print("="*80)
    
    # æŸ¥çœ‹æ‰€æœ‰å¿«ç…§
    print("\n1ï¸âƒ£ æŸ¥çœ‹è¡¨çš„å¿«ç…§å†å²:")
    snapshots = spark.sql(f"SELECT snapshot_id, committed_at, operation FROM {table_name}.snapshots")
    snapshots.show(truncate=False)
    
    # è·å–ç¬¬ä¸€ä¸ªå¿«ç…§ ID
    snapshot_ids = snapshots.select("snapshot_id").collect()
    if len(snapshot_ids) > 0:
        first_snapshot_id = snapshot_ids[0][0]
        
        print(f"\n2ï¸âƒ£ æŸ¥è¯¢ç¬¬ä¸€ä¸ªå¿«ç…§çš„æ•°æ® (snapshot_id={first_snapshot_id}):")
        df_history = spark.read \
            .option("snapshot-id", first_snapshot_id) \
            .format("iceberg") \
            .load(table_name)
        
        print(f"å†å²å¿«ç…§è®°å½•æ•°: {df_history.count()}")
        df_history.show(5)
        
        # ä½¿ç”¨æ—¶é—´æˆ³æŸ¥è¯¢
        print("\n3ï¸âƒ£ ä½¿ç”¨æ—¶é—´æˆ³æŸ¥è¯¢å†å²æ•°æ®:")
        spark.sql(f"""
            SELECT COUNT(*) as record_count, 
                   MIN(ts) as earliest_ts, 
                   MAX(ts) as latest_ts
            FROM {table_name}
            VERSION AS OF {first_snapshot_id}
        """).show()
    else:
        print("âš ï¸ æš‚æ— å¿«ç…§æ•°æ®")


def demo_schema_evolution(spark, table_name: str):
    """
    æ¼”ç¤º Schema Evolution åŠŸèƒ½
    
    Iceberg æ”¯æŒçµæ´»çš„æ¨¡å¼æ¼”å˜:
    - æ·»åŠ åˆ—
    - åˆ é™¤åˆ—
    - é‡å‘½ååˆ—
    - ä¿®æ”¹åˆ—ç±»å‹ï¼ˆå…¼å®¹çš„ç±»å‹ï¼‰
    """
    print("\n" + "="*80)
    print("ğŸ“‹ Schema Evolution æ¼”ç¤º")
    print("="*80)
    
    print("\n1ï¸âƒ£ å½“å‰è¡¨ç»“æ„:")
    spark.sql(f"DESCRIBE {table_name}").show(truncate=False)
    
    # æ·»åŠ æ–°åˆ—
    print("\n2ï¸âƒ£ æ·»åŠ æ–°åˆ— 'discount':")
    spark.sql(f"ALTER TABLE {table_name} ADD COLUMN discount DOUBLE")
    
    print("\næ·»åŠ åçš„è¡¨ç»“æ„:")
    spark.sql(f"DESCRIBE {table_name}").show(truncate=False)
    
    # æ›´æ–°æ–°åˆ—çš„å€¼
    print("\n3ï¸âƒ£ ä¸ºæ–°åˆ—è®¾ç½®é»˜è®¤å€¼:")
    spark.sql(f"""
        UPDATE {table_name}
        SET discount = CASE 
            WHEN amount > 1000 THEN 0.1
            WHEN amount > 500 THEN 0.05
            ELSE 0.0
        END
        WHERE discount IS NULL
    """)
    
    print("\næŸ¥çœ‹æ›´æ–°åçš„æ•°æ®:")
    spark.sql(f"SELECT order_id, amount, discount FROM {table_name} LIMIT 5").show()


def demo_merge_into(spark, table_name: str):
    """
    æ¼”ç¤º MERGE INTO åŠŸèƒ½
    
    Iceberg çš„ MERGE INTO æ”¯æŒ:
    - UPSERT æ“ä½œï¼ˆæ’å…¥æˆ–æ›´æ–°ï¼‰
    - åŸºäºæ¡ä»¶çš„æ›´æ–°å’Œåˆ é™¤
    - å¤æ‚çš„ CDC (Change Data Capture) åœºæ™¯
    """
    print("\n" + "="*80)
    print("ğŸ”„ MERGE INTO æ¼”ç¤º")
    print("="*80)
    
    # åˆ›å»ºä¸€äº›æ›´æ–°æ•°æ®
    print("\n1ï¸âƒ£ åˆ›å»ºæ›´æ–°æ•°æ®:")
    updates_data = [
        ("order_001", "user_001", "item_A", "Electronics", 999.99, 2, "2025-10-08 10:00:00"),
        ("order_999", "user_999", "item_Z", "Books", 29.99, 1, "2025-10-08 11:00:00"),
    ]
    
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType
    
    updates_schema = StructType([
        StructField("order_id", StringType(), False),
        StructField("user_id", StringType(), False),
        StructField("item_id", StringType(), False),
        StructField("category", StringType(), True),
        StructField("price", DoubleType(), True),
        StructField("quantity", IntegerType(), True),
        StructField("ts", TimestampType(), True),
    ])
    
    updates_df = spark.createDataFrame(updates_data, updates_schema) \
        .withColumn("ts", F.to_timestamp("ts")) \
        .withColumn("amount", F.col("price") * F.col("quantity")) \
        .withColumn("ingest_date", F.to_date(F.col("ts"))) \
        .withColumn("year", F.year(F.col("ts"))) \
        .withColumn("month", F.month(F.col("ts")))
    
    updates_df.show()
    
    # åˆ›å»ºä¸´æ—¶è§†å›¾
    updates_df.createOrReplaceTempView("updates")
    
    print("\n2ï¸âƒ£ æ‰§è¡Œ MERGE INTO:")
    print("è§„åˆ™: å¦‚æœ order_id å­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ’å…¥")
    
    spark.sql(f"""
        MERGE INTO {table_name} t
        USING updates u
        ON t.order_id = u.order_id
        WHEN MATCHED THEN
            UPDATE SET 
                t.price = u.price,
                t.quantity = u.quantity,
                t.amount = u.amount,
                t.ts = u.ts
        WHEN NOT MATCHED THEN
            INSERT *
    """)
    
    print("\nâœ… MERGE å®Œæˆï¼æŸ¥çœ‹ç»“æœ:")
    spark.sql(f"SELECT * FROM {table_name} WHERE order_id IN ('order_001', 'order_999')").show()


def demo_incremental_read(spark, table_name: str):
    """
    æ¼”ç¤ºå¢é‡è¯»å–åŠŸèƒ½
    
    Iceberg æ”¯æŒé«˜æ•ˆçš„å¢é‡è¯»å–:
    - åªè¯»å–è‡ªä¸Šæ¬¡è¯»å–ä»¥æ¥çš„æ–°æ•°æ®
    - ç”¨äºå¢é‡ ETL ç®¡é“
    """
    print("\n" + "="*80)
    print("ğŸ“– å¢é‡è¯»å–æ¼”ç¤º")
    print("="*80)
    
    # è·å–å¿«ç…§ä¿¡æ¯
    snapshots = spark.sql(f"SELECT snapshot_id, committed_at FROM {table_name}.snapshots ORDER BY committed_at")
    snapshot_ids = snapshots.select("snapshot_id").collect()
    
    if len(snapshot_ids) >= 2:
        start_snapshot = snapshot_ids[0][0]
        end_snapshot = snapshot_ids[-1][0]
        
        print(f"\n1ï¸âƒ£ è¯»å–å¿«ç…§ {start_snapshot} åˆ° {end_snapshot} ä¹‹é—´çš„å¢é‡æ•°æ®:")
        
        incremental_df = spark.read \
            .format("iceberg") \
            .option("start-snapshot-id", start_snapshot) \
            .option("end-snapshot-id", end_snapshot) \
            .load(table_name)
        
        print(f"å¢é‡æ•°æ®è®°å½•æ•°: {incremental_df.count()}")
        incremental_df.show(10)
    else:
        print("âš ï¸ å¿«ç…§æ•°é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ 2 ä¸ªå¿«ç…§æ‰èƒ½æ¼”ç¤ºå¢é‡è¯»å–")


def demo_maintenance(spark, table_name: str):
    """
    æ¼”ç¤ºè¡¨ç»´æŠ¤æ“ä½œ
    
    Iceberg çš„ç»´æŠ¤æ“ä½œ:
    - OPTIMIZE: åˆå¹¶å°æ–‡ä»¶ï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½
    - EXPIRE SNAPSHOTS: åˆ é™¤è¿‡æœŸå¿«ç…§ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´
    - REWRITE DATA FILES: é‡å†™æ•°æ®æ–‡ä»¶ä»¥ä¼˜åŒ–å¸ƒå±€
    """
    print("\n" + "="*80)
    print("ğŸ”§ è¡¨ç»´æŠ¤æ“ä½œæ¼”ç¤º")
    print("="*80)
    
    # æŸ¥çœ‹å½“å‰æ–‡ä»¶çŠ¶æ€
    print("\n1ï¸âƒ£ å½“å‰æ•°æ®æ–‡ä»¶çŠ¶æ€:")
    files_df = spark.sql(f"""
        SELECT 
            COUNT(*) as file_count,
            SUM(record_count) as total_records,
            AVG(file_size_in_bytes) as avg_file_size,
            SUM(file_size_in_bytes) as total_size
        FROM {table_name}.files
    """)
    files_df.show()
    
    # æ‰§è¡Œ OPTIMIZEï¼ˆåˆå¹¶å°æ–‡ä»¶ï¼‰
    print("\n2ï¸âƒ£ æ‰§è¡Œ OPTIMIZE åˆå¹¶å°æ–‡ä»¶:")
    spark.sql(f"CALL local.system.rewrite_data_files(table => '{table_name}')")
    
    print("\nä¼˜åŒ–åçš„æ–‡ä»¶çŠ¶æ€:")
    files_df = spark.sql(f"""
        SELECT 
            COUNT(*) as file_count,
            SUM(record_count) as total_records,
            AVG(file_size_in_bytes) as avg_file_size,
            SUM(file_size_in_bytes) as total_size
        FROM {table_name}.files
    """)
    files_df.show()
    
    # æŸ¥çœ‹å¿«ç…§æ•°é‡
    print("\n3ï¸âƒ£ å½“å‰å¿«ç…§æ•°é‡:")
    snapshot_count = spark.sql(f"SELECT COUNT(*) as count FROM {table_name}.snapshots")
    snapshot_count.show()
    
    print("\nğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ EXPIRE SNAPSHOTS åˆ é™¤è¿‡æœŸå¿«ç…§:")
    print(f"   CALL local.system.expire_snapshots(table => '{table_name}', older_than => TIMESTAMP '2025-10-01 00:00:00')")


def main():
    args = parse_args()
    spark = build_spark("IcebergFeatures", local=bool(args.local), enable_iceberg=True)
    
    table_name = args.table
    feature = args.feature
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    tables = spark.sql("SHOW TABLES").collect()
    table_exists = any(table_name.split(".")[-1] in str(row) for row in tables)
    
    if not table_exists:
        print(f"âš ï¸ è¡¨ {table_name} ä¸å­˜åœ¨ï¼è¯·å…ˆè¿è¡Œ ingest_iceberg.py åˆ›å»ºè¡¨ã€‚")
        spark.stop()
        return
    
    # æ ¹æ®é€‰æ‹©æ¼”ç¤ºä¸åŒç‰¹æ€§
    if feature == "all" or feature == "time_travel":
        demo_time_travel(spark, table_name)
    
    if feature == "all" or feature == "schema_evolution":
        demo_schema_evolution(spark, table_name)
    
    if feature == "all" or feature == "merge":
        demo_merge_into(spark, table_name)
    
    if feature == "all" or feature == "incremental":
        demo_incremental_read(spark, table_name)
    
    if feature == "all" or feature == "maintenance":
        demo_maintenance(spark, table_name)
    
    print("\n" + "="*80)
    print("âœ… Iceberg ç‰¹æ€§æ¼”ç¤ºå®Œæˆï¼")
    print("="*80)
    
    spark.stop()


if __name__ == "__main__":
    main()

