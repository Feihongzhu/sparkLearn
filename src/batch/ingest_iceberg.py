"""
Iceberg æ•°æ®æ‘„å…¥ç¤ºä¾‹ï¼šCSV -> Iceberg è¡¨
æ¼”ç¤º Iceberg çš„åŸºæœ¬ä½¿ç”¨ã€ACID äº‹åŠ¡ã€Schema Evolutionã€Time Travel ç­‰ç‰¹æ€§
"""
import argparse
from pathlib import Path

from pyspark.sql import functions as F
from pyspark.sql import types as T

from src.common.spark_session import build_spark


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="CSV -> Iceberg table ingest job")
    parser.add_argument("--input", required=True, help="Input CSV file path")
    parser.add_argument("--table", default="local.db.orders", help="Iceberg table name (catalog.database.table)")
    parser.add_argument("--local", action="store_true", help="Use local[*] Spark for debugging")
    parser.add_argument("--mode", default="append", choices=["append", "overwrite"], help="Write mode")
    return parser.parse_args()


def read_csv_with_schema(spark, input_path: str):
    """è¯»å– CSV æ–‡ä»¶å¹¶å®šä¹‰ Schema"""
    schema = T.StructType([
        T.StructField("order_id", T.StringType(), False),
        T.StructField("user_id", T.StringType(), False),
        T.StructField("item_id", T.StringType(), False),
        T.StructField("category", T.StringType(), True),
        T.StructField("price", T.DoubleType(), True),
        T.StructField("quantity", T.IntegerType(), True),
        T.StructField("ts", T.TimestampType(), True),
    ])
    return (
        spark.read
        .option("header", True)
        .schema(schema)
        .csv(input_path)
    )


def transform(df):
    """æ•°æ®è½¬æ¢ï¼šæ·»åŠ è®¡ç®—å­—æ®µå’Œåˆ†åŒºå­—æ®µ"""
    return (
        df.withColumn("amount", F.col("price") * F.col("quantity"))
          .withColumn("ingest_date", F.to_date(F.col("ts")))
          .withColumn("year", F.year(F.col("ts")))
          .withColumn("month", F.month(F.col("ts")))
    )


def write_to_iceberg(spark, df, table_name: str, mode: str = "append"):
    """
    å†™å…¥ Iceberg è¡¨
    
    Iceberg çš„ç‰¹ç‚¹ï¼š
    1. è‡ªåŠ¨ ACID äº‹åŠ¡
    2. éšè—åˆ†åŒºï¼ˆç”¨æˆ·æ— éœ€æ‰‹åŠ¨æŒ‡å®šåˆ†åŒºè·¯å¾„ï¼‰
    3. Schema Evolution æ”¯æŒ
    """
    # ç¡®ä¿æ•°æ®åº“å­˜åœ¨
    db_name = ".".join(table_name.split(".")[:-1])
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")
    
    # å†™å…¥ Iceberg è¡¨
    (
        df.write
        .format("iceberg")
        .mode(mode)
        # Iceberg ä½¿ç”¨éšè—åˆ†åŒºï¼Œåœ¨è¡¨åˆ›å»ºæ—¶å®šä¹‰
        .save(table_name)
    )
    
    print(f"âœ… æˆåŠŸå†™å…¥ Iceberg è¡¨: {table_name}")


def create_partitioned_table(spark, table_name: str):
    """
    åˆ›å»ºåˆ†åŒº Iceberg è¡¨
    
    Iceberg çš„éšè—åˆ†åŒºç‰¹æ€§ï¼š
    - ç”¨æˆ·æŸ¥è¯¢æ—¶ä¸éœ€è¦çŸ¥é“åˆ†åŒºå­—æ®µ
    - è‡ªåŠ¨è¿›è¡Œåˆ†åŒºè£å‰ªä¼˜åŒ–
    """
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            order_id STRING,
            user_id STRING,
            item_id STRING,
            category STRING,
            price DOUBLE,
            quantity INT,
            ts TIMESTAMP,
            amount DOUBLE,
            ingest_date DATE,
            year INT,
            month INT
        )
        USING iceberg
        PARTITIONED BY (days(ts))
        TBLPROPERTIES (
            'write.format.default' = 'parquet',
            'write.parquet.compression-codec' = 'snappy'
        )
    """)
    print(f"âœ… åˆ›å»º Iceberg è¡¨: {table_name}")


def main():
    args = parse_args()
    spark = build_spark("IngestIceberg", local=bool(args.local), enable_iceberg=True)
    
    input_path = args.input
    table_name = args.table
    mode = args.mode
    
    # åˆ›å»º Iceberg è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    create_partitioned_table(spark, table_name)
    
    # è¯»å–å’Œè½¬æ¢æ•°æ®
    df = read_csv_with_schema(spark, input_path)
    df_out = transform(df)
    
    # å†™å…¥ Iceberg è¡¨
    write_to_iceberg(spark, df_out, table_name, mode)
    
    # æ˜¾ç¤ºè¡¨çš„å…ƒæ•°æ®
    print("\nğŸ“Š è¡¨çš„å¿«ç…§å†å²:")
    spark.sql(f"SELECT * FROM {table_name}.snapshots").show(truncate=False)
    
    print("\nğŸ“ è¡¨çš„æ•°æ®æ–‡ä»¶:")
    spark.sql(f"SELECT file_path, record_count, file_size_in_bytes FROM {table_name}.files").show(truncate=False)
    
    print("\nğŸ“ˆ è¡¨çš„ç»Ÿè®¡ä¿¡æ¯:")
    spark.sql(f"DESCRIBE EXTENDED {table_name}").show(truncate=False)
    
    spark.stop()


if __name__ == "__main__":
    main()

